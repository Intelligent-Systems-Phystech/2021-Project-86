\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\usepackage{graphicx} 
\graphicspath{ {./figures/} }
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfig}
\newcommand{\hdir}{.}
\renewcommand{\baselinestretch}{1.35}

\begin{document}
\English
\title
	[] % short title for page headings, not necessary if a full title fits the headings
    { Learning co-evolution information with natural language processing for protein folding problem} % full title
\author
	[F.\,S.~Author] % short list of the authors (<= 3) for page headings, is necessary only if the full list does not fit the headings
	{F.\,S.~Author, F.\,S.~Co-Author, and F.\,S.~Name} % full list of the authors, presented in the table of contetns of the issue
    [Egor Zverev$^1$, Sergei Grudinin$^2$, and Ilia Igashov$^{1, 2}$] % list of the authors presented in the title page of the article, is necessary only if it differs from the full list of the authors in braces, i.e. '{' and '}'
\email
    {zverev.eo@phystech.edu; sergei.grudinin@inria.fr; igashov.is@phystech.edu}
\thanks
    {}
\organization
    {$^1$Organization, address;
     $^2$Organization, address}
\abstract
    {Co-evolution information is crucial for building protein sequence descriptors. Its computation is traditionally based on Multiple Sequence Alignments. Applications of MSA are limited, for instance, it fails for sequences with shallow alignment. In this work, we investigate pre-trained language models  as a potential alternative to MSA. Specifically, our main focus is fold classification problem. We examine several  state-of-the-art  methods  and  modify  them  by  replacing  MSA-based  feature-generation parts with pre-trained LMs. We compare language models and MSA in their ability to extract sequence evolution information.

		
	\noindent
    	\textbf{Keywords}: \emph{protein fold classification; co-evolution, feature generation; transformers; BERT;  }}




%these fields are filled in by the journal editors
\doi{10.21469/22233792}
\receivedRus{01.01.2017}
\receivedEng{January 01, 2017}

\maketitle
\linenumbers

\section{Introduction}
\noindent %this command is placed at the beginning of the first sentence of each paragraph/section only.




Protein properties are determined by its shape, which is specified by its amino acid sequence \cite{cellbiology}. Therefore it is important to learn how to analyze this sequence.
Any analysis[...] begins by constructing protein sequence descriptors. Their essential part is a co-evolution information. This information is obtained by searching for homologues of the protein in large databases and further computing multiple sequence alignment (MSA) on it. This technique requires significant computational efforts, moreover, it does not guarantee precise results since it fully relies on finite databases. Besides, this method fails for sequences with shallow alignment 

 Co-evolution-based methods assume that a mutation of one amino acid leads to mutations of others. Therefore, given an amino acid sequence it is natural to look for other similar sequences. That is performed with MSA \cite{co-evolution}. This method fails when we are dealing with the sequences that did not evolve a lot. They have shallow alignments.There are only a few similar sequences in the database. Therefore, information extracted from these alignments is insignificant. 

Computing MSA is a heavy task. Alignment databases are finite. If for a given sequence there are no matches in the database, MSA-based methods fail to produce reliable results. 

In this work, we analyse pre-trained language models (LMs)  \cite{Elnaggar} as a potential alternative to traditional MSA approaches. It is assumed that protein sequences are not random \cite{evolution-not-random}, that there is logic in amino acids structure. Therefore, a set of all amino acids could be seen as a language with complicated inner rules. With the invention of BERT \cite{bert} it has become possible to learn the structure of any abstract language. It is natural to assume that by learning amino acids structure, BERT will be able to implicitly learn co-evolution information.

Our main aim is to study the efficiency of pre-trained LMs in application to the fold classification problem.  To start with, we consider the state-of-the-art method DeepSF \cite{DeepSF}. It solves the protein fold classification problem.  By replacing its MSA-based feature-generation part with a pre-trained LM, we study how NLP approach affects learning the fold-related information.  The whole framework is schematically represented in Figure 1. Further, we perform the same with other fold classification algorithms \cite{Villegas, DeepFrag}
as well as with end-to-end protein structure prediction methods \cite{Kandathil, Xu2020.10.12.336859}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{figures/figure1.pdf}
    \caption{The main framework}
\end{figure}
\section{Problem statement}
\paragraph{General description}
\noindent
Proteins that share similarities in their structure are  divided into classes -- folds \cite{Schaeffer2011}. In this work we investigate a fold classification problem.  

We use the following notation: $Y$ is a set of all known classes, $A$ is a set of all proteins with known folds and  $f: A \rightarrow Y$ is a mapping between proteins and their classes. 

Suppose, $x \not\in A$ is a protein with unknown fold. We aim to build a model capable of classifying $x$ as an element of one of the present classes, extending $f$ on the set $A \cup \{x\}$. 

\paragraph{Feature generation }
\noindent

In the fold classification problem $x_i$ represents i-th protein description in the $D$-dimensional space. However, initially each protein is described by its amino acid sequence. Let $\Sigma$ be a 20-letter alphabet of acid sequences.  

We are given a set of $A = \{a_i \in \Sigma^+\}_{i = 1}^n$. Let $g_{\theta, D}: \Sigma^+ \rightarrow \mathbb{R}^D$ be an embedding of the space of acid sequences into D-dimensional set of descriptors. These embeddings are parameterized  by $\theta$.

We separate $A$ into training and validation set. Training set is used to formulate fold classification problem. Validation set is used to formulate feature generation problem. 

$A = A_{train} \cup A_{val}$ 

Protein sequence descriptors are generated using $g_{\theta, D}$ as $x_i = g_{\theta, D}(a_i)$ 

\paragraph{Fold classification problem}
\noindent

Suppose there is a given set of pairs $S = \{ (x_i, Y_i)\}_{i = 1}^n$ where $ x_i \in \mathbb{R}^D$ denotes the sequence descriptor of i-th protein and $ Y_i \in \mathbb{Y} = \{ (1, 0, ..., 0), (0, 1, 0, ..., 0) .... (0, ..., 0, 1) \}$ denotes the i-th protein known class, $|\mathbb{Y}| = m$

We separate $S$ into training and validation set. Training set is used to formulate fold classification problem.  

$S = S_{train} \cup S_{val}$

Let $W = \{(w_1, ..., w_m)| \forall i \in \{1...m\} w_i \in \mathbb{R}^D\}$ be a space of parameters for classification models. Let $g_w$ be a model parameterized by $w$.
We use $e_k$ to denote a vector with 1 on k-th position, with zeros on all other positions.
\[
\begin{matrix}
e_{k} = &
        &(0,\ldots,0,&1,&0,\dots,0)\\
        &&&\uparrow&      \\
        &&&k&           
\end{matrix}
\]
We work under the following assumption:  

$P_w(Y = e_k| x) = \frac{exp(x^Tw_k)}{\sum_j exp(x^t w_j)}$

Let $p_{w, x}(y)$ be discrete density of $Y$, $L_{Y, x}(w)$ be likelihood function of $Y$.  

$p_{w, x}(y) = \prod_{d = 1}^{m} (P_w(Y = e_d| x))^{y_d}$, 

$L_{Y, x}(w) = \prod_{i = 1}^{n} p_{w, x}(Y_i)$

Given $S_{train}$, classification problem is a maximization of likelihood on training set: 

$L_{train}(w) = L_{Y_{train}, x_{train}}(w) \rightarrow \max\limits_{w \in W}$

\paragraph{Feature generation problem}
\noindent

For fixed $\theta, D$ and descriptors $x_i$ let's denote $\hat{w} = \argmax\limits_{w \in W} L_{Y_{train}, x_{train}}(w)$.  

Feature generation problem is a maximization of likelihood function on validation set:  

$L_{Y_{val}, x_{val}}(\hat{w}) \rightarrow \max\limits_{\theta, D}$

\paragraph{Overview}
\noindent

The problem of determining proteins' class is separated into two parts: first, we encode amino acid sequences as $D$-dimensional descriptors, then these descriptors are used to train a prediction model.  

\section{Computational experiment}


\paragraph{Data}
\noindent
We use SCOP2 \cite{SCOP2FST, SCOP2SND} dataset. It contains information about proteins whose structure is already known. All the proteins in the database have classes assigned to them (folds).  At present, there are 1501 known folds \cite{SCOP2SND}. The number of proteins contained in each fold is less than 50 for most of the folds. Detailed histogram is represented in Figures 2, 3. 

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{figures/figure2.pdf}
    \caption{Protein count histogram}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{figures/figure3.pdf}
    \caption{Protein count zoomed histogram}
\end{figure}

In SCOP2 proteins are represented by their amino acid sequences. Each sequence is stored as a string in the 20 letter alphabet and an ID. The length of the string varies mostly between 25 and 500 from protein to another. The length diagram is represented on Figure 2. Search by ID allows database users to extract information about target proteins, including their fold classes. 

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{figures/figure4.pdf}
    \caption{Sequence length histogram}
\end{figure}

\paragraph{Baseline solution}
\noindent
Our solution is a modification of the DeepSF \cite{DeepSF} method. The designers of DeepSF use PSI-BLAST\cite{PSIBLAST} to generate features from MSA  as well as SCRATCH \cite{SCRATCH} to extract secondary structure (3 classes) and solvent accessibility (2 classes).  In the core of this classification method PSI-BLAST is used during the first stage of prediction \cite{SCRATCH}. 

Hoe et al. then use these features as an input to the deep convolutional neural network. The depth of CNN used in DeepSF is 10. The output of the model is a vector of fold probabilities. Fold with the maximal probability is accepted as a target protein fold. 

\paragraph{Proposed solution}
\noindent

DeepSF feature generation is entirely based on MSA. The main point of this work is to suggest another approach to feature generation which is independent of MSA. 

We apply BERT to the language of amino acids. We take the representation of acid sequences BERT creates as a new set of features. Then we train DeepSF model on these features.

It shall be noted that we do not reject co-evolution information in our research. Instead of directly applying MSA we let BERT learn information about amino acid language. We suppose that the model learns evolution information implicitly.

\paragraph{Evaluation}
\noindent
We use standard cross-validation procedure to compute accuracy of our model. We then compare it to the accuracy score of DeepSF method based on MSA feature generation as well as several other state-of-the-art techniques. 

\section{Structure of the article}
\noindent
Divide your article into clearly defined and numbered sections and paragraphs.

\paragraph{Paragraph}
\noindent
Sections and paragraphs are numbered and have a brief heading.

%please do not change the name of this section if it is present
\section{Concluding Remarks}
This section should provide the summary and explore the significance of the results achieved and list problems not yet solved.
Results should be clear and concise. 

%%%% please specify doi of the cited item if possible, see~\bibitem{article}
%%%% Crossref doi of the item can be retrieved at http://www.crossref.org/guestquery/
\bibliographystyle{plain}
\bibliography{Zverev2020CoevolutionFromLMs}

%begin{thebibliography}{99}



% \bibitem{book}
% 	\BibAuthor{Goossens,~M., F. Mittelbach, and A.~Samarin}. 1994.
% 	\BibTitle{The \LaTeX\ companion}.
% 	2nd ed.
% 	Reading, MA: Addison-Wesley. 528 p.

% \bibitem{article}
% 	\BibAuthor{Zagurenko,~A.\,G., V.\,A.~Korotovskikh, A.\,A.~Kolesnikov, A.\,V.~Timonov, and D.\,V.~Kardymon}. 2008.
% 	Tekhniko-ekonomicheskaya optimizatsiya dizayna gidrorazryva plasta
% 	[Technical and economic optimization of the design of hydraulic fracturing].
% 	\BibJournal{Neftyanoe Khozyaystvo} [Oil Industry] 11(1):54--57.
% 	\BibDoi{10.3114/S187007708007}. (In Russian)

% \bibitem{webArticle}
% 	\BibAuthor{Blaga,~P.\,A.} 2007.
% 	Commutative Diagrams with XY-pic II. Frames and Matrices.
% 	\BibJournal{PracTEX J.}  4.
% 	Available at: \BibUrl{https://tug.org/pracjourn/2007-1/blaga/blaga.pdf}
%     (accessed February 20, 2007).

% \bibitem{webResource}
% 	XYpic.
% 	Available at: \BibUrl{http://akagi.ms.u-tokyo.ac.jp/input9.pdf}
% 	(accessed April 09, 2015).

% \bibitem{inproceedingsRus}
% 	\BibAuthor{Usmanov,~T.\,S., A.\,A.~Gusmanov, I.\,Z.~Mullagalin, R.\,Yu.~Mukhametshina, A.\,N.~Chervyakova, and A.\,V.~Sveshnikov.} 2007.
% 	Osobennosti proektirovaniya razrabotki mestorozhdeniy s primeneniem gidrorazryva plasta
% 	[Features of the design of field development with the use of hydraulic fracturing].
% 	\BibJournal{6th Symposium (International) ``New Energy Saving Subsoil Technologies and the
% 	Increasing of the Oil and Gas Impact'' Proceedings}.
% 	Moscow:~Publisher. 267--272. (In Russian)
	   	
% \bibitem{inproceedingsEng}
%     \BibAuthor{Author,~N.} 2009.
%     Paper title.
%     \BibJournal{10th Conference (International) on Any Science Proceedings}.
%     Place of publication: Publisher. 111--122.
	
% \bibitem{techreport}
% 	\BibAuthor{Lambert,~P.} 1993.
%   	\BibTitle{The title of the work}.
%   	Place of publication:~The institution that published.  Report~2.
  	     	
%end{thebibliography}



\end{document}
