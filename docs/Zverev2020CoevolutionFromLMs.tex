\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\newcommand{\hdir}{.}
\renewcommand{\baselinestretch}{1.35}

\begin{document}
\English
\title
	[] % short title for page headings, not necessary if a full title fits the headings
    { Learning co-evolution information with natural language processing for protein folding problem} % full title
\author
	[F.\,S.~Author] % short list of the authors (<= 3) for page headings, is necessary only if the full list does not fit the headings
	{F.\,S.~Author, F.\,S.~Co-Author, and F.\,S.~Name} % full list of the authors, presented in the table of contetns of the issue
    [Egor Zverev$^1$, Sergei Grudinin$^2$, and Ilia Igashov$^{1, 2}$] % list of the authors presented in the title page of the article, is necessary only if it differs from the full list of the authors in braces, i.e. '{' and '}'
\email
    {zverev.eo@phystech.edu; sergei.grudinin@inria.fr; igashov.is@phystech.edu}
\thanks
    {}
\organization
    {$^1$Organization, address;
     $^2$Organization, address}
\abstract
    {Many problems in bioinformatics are related to protein folding. Most of the techniques aimed to solve such problems usually begin by building protein sequence descriptors. One of the most significant parts of such descriptors is co-evolution information which computation is based on Multiple Sequence Alignments (MSA). This technique requires significant computational efforts, moreover, it does not guarantee precise results since it fully relies on finite databases. Besides, this method fails for sequences with shallow alignment. In this work, we will consider pre-trained language models (LMs)  as a potential alternative to this increasingly time-consuming database search.  Specifically, our main focus is fold classification problem. We will examine several  state-of-the-art  methods  and  modify  them  by  replacing  MSA-based  feature-generation parts with pre-trained LMs.

		
	\noindent
    	\textbf{Keywords}: \emph{fold classification; co-evolution, NLP; transformers; BERT;  }}




%these fields are filled in by the journal editors
\doi{10.21469/22233792}
\receivedRus{01.01.2017}
\receivedEng{January 01, 2017}

\maketitle
\linenumbers

\section{Introduction}
\noindent %this command is placed at the beginning of the first sentence of each paragraph/section only.
Protein properties are determined by its shape which is specified by its amino acid sequence \cite{cellbiology}. Therefore it is important to learn how to analyze this sequence.
Any analysis usually begins by constructing protein sequence descriptors whose essential part is a co-evolution information. This information can be obtained by searching for homologues of the target protein in large databases and further computing multiple sequence alignment (MSA) on it. 

 Co-evolution-based methods assume that a mutation of one amino acid very often leads to mutations of others. Therefore given an amino acid sequence it is natural to look for other similar sequences. That is usually performed with MSA \cite{co-evolution}. The problem arises when weâ€™re dealing with the sequences that did not evolve a lot. They will have shallow alignments, i.e. there will be only a few similar sequences. Therefore, information extracted from these alignments will be insignificant. 

Besides, computing MSA is a heavy task. Moreover, alignment databases are finite. If for a given sequence there are no matches in the database, MSA-based methods fail to produce reliable results. 

In this work, we will consider pre-trained language models (LMs)  \cite{Elnaggar} as a potential alternative to traditional MSA approaches. It is assumed that protein sequences are not random \cite{evolution-not-random}, that there is logic in amino acids structure. Therefore, a set of all amino acids could be seen as a language with complicated inner rules. With the invention of BERT \cite{bert} it has become possible to learn the structure of any abstract language. It is natural to assume that by learning amino acids structure, BERT will be able to implicitly learn co-evolution information.

Our main aim is to study the efficiency of pre-trained LMs in application to the fold classification problem.  To start with, we will consider the state-of-the-art method DeepSF \cite{DeepSF} which solves the protein fold classification problem.  By replacing its MSA-based feature-generation part with a pre-trained LM, we will study how NLP approach affects on learning the fold-related information.  The whole framework is schematically represented in Figure 1. Further, we can do the same with other fold classification algorithms \cite{Villegas, DeepFrag}
as well as with end-to-end protein structure prediction methods \cite{Kandathil, Xu2020.10.12.336859}.



\section{Problem statement}
\paragraph{General description}
\noindent
Proteins that share similarities in their structure are  divided into classes -- folds \cite{Schaeffer2011}. In this work we investigate a fold classification problem.  

We use the following notation: $S$ is a set of all known classes, $X$ is a set of all proteins with known folds and  $f: X \rightarrow S$ is a mapping between proteins and their classes. 

Suppose, $x \not\in X$ is a protein with unknown fold. We aim to build a model capable of either classifying $x$ as an element of one of the present classes, extending $f$ on the set $X \cup \{x\}$ or concluding that $x$ does not share enough similarities with any of known classes. 

\paragraph{Data}
\noindent
We use SCOP2 \cite{SCOP2FST, SCOP2SND} dataset. It contains information about proteins whose structure is already known. All the proteins in the database have classes assigned to them (folds).  At present, there are 1501 known folds \cite{SCOP2SND}. 

In SCOP2 proteins are represented by their amino acid sequences. Each sequence is stored as a string in the 20 letter alphabet and an ID. The length of the string varies from one protein to another. Search by ID allows database users to extract information about target proteins, including their fold classes. 

\paragraph{Baseline solution}
\noindent
Our solution is a modification of the DeepSF \cite{DeepSF} method. The designers of DeepSF use PSI-BLAST\cite{PSIBLAST} to generate features from MSA  as well as SCRATCH \cite{SCRATCH} to extract secondary structure (3 classes) and solvent accessibility (2 classes).  In the core of this classification method PSI-BLAST is used during the first stage of prediction \cite{SCRATCH}. 

Hoe et al. then use these features as an input to the deep convolutional neural network. The depth of CNN used in DeepSF is 10. The output of the model is a vector of fold probabilities. Fold with the maximal probability is accepted as a target protein fold. 

\paragraph{Proposed solution}
\noindent

DeepSF feature generation is entirely based on MSA. The main point of this work is to suggest another approach to feature generation which is independent of MSA. 

We apply BERT to the language of amino acids. We take the representation of acid sequences BERT creates as a new set of features. Then we train DeepSF model on these features

It shall be noted that we do not reject co-evolution information in our research. Instead of directly applying MSA we let BERT learn information about amino acid language. We suppose that the model learns evolution information implicitly.

\paragraph{Evaluation}
\noindent
We use standard cross-validation procedure to compute accuracy of our model. We then compare it to the accuracy score of DeepSF method based on MSA feature generation as well as several other state-of-the-art techniques. 

\section{Structure of the article}
\noindent
Divide your article into clearly defined and numbered sections and paragraphs.

\paragraph{Paragraph}
\noindent
Sections and paragraphs are numbered and have a brief heading.

%please do not change the name of this section if it is present
\section{Concluding Remarks}
This section should provide the summary and explore the significance of the results achieved and list problems not yet solved.
Results should be clear and concise. 

%%%% please specify doi of the cited item if possible, see~\bibitem{article}
%%%% Crossref doi of the item can be retrieved at http://www.crossref.org/guestquery/
\bibliographystyle{plain}
\bibliography{Zverev2020CoevolutionFromLMs}

%begin{thebibliography}{99}



% \bibitem{book}
% 	\BibAuthor{Goossens,~M., F. Mittelbach, and A.~Samarin}. 1994.
% 	\BibTitle{The \LaTeX\ companion}.
% 	2nd ed.
% 	Reading, MA: Addison-Wesley. 528 p.

% \bibitem{article}
% 	\BibAuthor{Zagurenko,~A.\,G., V.\,A.~Korotovskikh, A.\,A.~Kolesnikov, A.\,V.~Timonov, and D.\,V.~Kardymon}. 2008.
% 	Tekhniko-ekonomicheskaya optimizatsiya dizayna gidrorazryva plasta
% 	[Technical and economic optimization of the design of hydraulic fracturing].
% 	\BibJournal{Neftyanoe Khozyaystvo} [Oil Industry] 11(1):54--57.
% 	\BibDoi{10.3114/S187007708007}. (In Russian)

% \bibitem{webArticle}
% 	\BibAuthor{Blaga,~P.\,A.} 2007.
% 	Commutative Diagrams with XY-pic II. Frames and Matrices.
% 	\BibJournal{PracTEX J.}  4.
% 	Available at: \BibUrl{https://tug.org/pracjourn/2007-1/blaga/blaga.pdf}
%     (accessed February 20, 2007).

% \bibitem{webResource}
% 	XYpic.
% 	Available at: \BibUrl{http://akagi.ms.u-tokyo.ac.jp/input9.pdf}
% 	(accessed April 09, 2015).

% \bibitem{inproceedingsRus}
% 	\BibAuthor{Usmanov,~T.\,S., A.\,A.~Gusmanov, I.\,Z.~Mullagalin, R.\,Yu.~Mukhametshina, A.\,N.~Chervyakova, and A.\,V.~Sveshnikov.} 2007.
% 	Osobennosti proektirovaniya razrabotki mestorozhdeniy s primeneniem gidrorazryva plasta
% 	[Features of the design of field development with the use of hydraulic fracturing].
% 	\BibJournal{6th Symposium (International) ``New Energy Saving Subsoil Technologies and the
% 	Increasing of the Oil and Gas Impact'' Proceedings}.
% 	Moscow:~Publisher. 267--272. (In Russian)
	   	
% \bibitem{inproceedingsEng}
%     \BibAuthor{Author,~N.} 2009.
%     Paper title.
%     \BibJournal{10th Conference (International) on Any Science Proceedings}.
%     Place of publication: Publisher. 111--122.
	
% \bibitem{techreport}
% 	\BibAuthor{Lambert,~P.} 1993.
%   	\BibTitle{The title of the work}.
%   	Place of publication:~The institution that published.  Report~2.
  	     	
%end{thebibliography}



\end{document}
